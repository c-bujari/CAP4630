{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPt5Ub6KAeM8GcyaEzvv+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-bujari/CAP4630/blob/master/HW_5/HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30zHFsaCN0p7",
        "colab_type": "text"
      },
      "source": [
        "# Homework 5\n",
        "###CAP 4630 Artificial Intelligience\n",
        "####Clyde Bujari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqH-DACEN3IK",
        "colab_type": "text"
      },
      "source": [
        "## 1. General Concepts\n",
        "\n",
        "**Artificial Intelligience** is a concept that has had many definitions by many different people over the years - but the most accurate one is John McCarthy's definition of it as \"the science and engineering of making intelligient machines\". \n",
        "\n",
        "In other words, the study of AI is the study of designing computer systems which can perform tasks such as visual perception, speech recognition, decision making, etc.; tasks which are virtually impossible to emulate with traditional computer programming, but that human intelligience is naturally good at.\n",
        "\n",
        "**Machine Learning** is a subset of AI. Arthur Samuel defined it as a \"field of study that gives computers the ability to learn without being explicitely programmed\". The goal of ML is to create programs, more specifically, which adjust their output in response to a given set of input data, much like humans learn from birth. These programs are more flexible than those created by human programmers, able to change themselves based on data and thus create much more feasible solutions to problems AI seeks to solve.\n",
        "\n",
        "**Deep Learning** is a subset of ML that specifically uses algorithms broken into  multiple layers (hence the term \"deep\"). By breaking up an algorithm into these layers, each layer can handle a different part of the process - for example, in image processing a lower layer may handle edge identification, and a higher layer may identify which of these edges correspond to a face or a digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDjj0gKsOABH",
        "colab_type": "text"
      },
      "source": [
        "## 2. Basic Concepts\n",
        "\n",
        "**Linear regression** is a statistical tool used to model the relationship between a dependent variable and one or more independent variables.\n",
        "\n",
        "**Logistic regression**, is a statistical tool that is used to model the probability of a certain class/event being valid - pass/fail, win/lose, etc. This can also be extended to more than just two binary classes, such as to determine what animal an image contains (assigning each possible class a probability between 0 and 1).\n",
        "\n",
        "The **gradient** of a multivariate function is a vector field for which the components of a vector at a given point are the partial derivatives of function f. It is the multivariate equivalent to a \"derivative\"; the gradient vector's direction = the direction of fastest increase of f, and the gradient vector's magnitude = the rate of increase in that direction.\n",
        "\n",
        "**Gradient descent** (also known as *steepest descent*) is an iterative algorithm for finding a local minimum of any differentiable function. In each iteration, it updates the value of this minimum by decreasing by a step size, where the step = the gradient at the current point ${*}$ a scalar learning rate.\n",
        "\n",
        "Gradient descent can be computationally expensive. To mitigate this, we can break our dataset into much smaller batches - approaching the same amount of accuracy with much less computation over many iterations. **Stochastic gradient descent (SGD)** is an extreme example of this, computing single samples from the dataset in each iteration to great performance improvements, but at the cost of noise. \n",
        "\n",
        "A middle ground between this and computing the full batch at once is **Mini-batch stochastic gradient descent (Mini-Batch SGD)**, which uses batch sizes of 10 - 1000. This produces much less noise than SGD, while still being more efficient than a full-batch gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0ioUn6eIxSV",
        "colab_type": "text"
      },
      "source": [
        "## 3. Building a model\n",
        "\n",
        "A typical Keras model generally consists of several common types of layers appropriate for different use cases:\n",
        "* **Fully connected layers (`keras.Dense()`):**\n",
        "\n",
        "These layers consist of simple vector data stored in 2D tensors of shape (samples, features). Also called *densely connected layers*, this is the typical set of weights and biases that form the foundation of all models.\n",
        "* **Recurrent layers:**\n",
        "\n",
        "These layers store sequence data in 3D tensors of shape (samples, timesteps, features). In other words they use the *sequence in which data occurs* in addition to the data itself, very useful for tasks such as speech recognition.\n",
        "* **Convolutional layers (`keras.Conv2d()`):**\n",
        "\n",
        "These layers typically process image data, stored in 4D tensors. They perform a convolution operation on this data, applying a filter to the matrix of data to identify image features. The output of these layers must be flattened before being passed to different types of layers, such as the fully connected layers that almost always follow them.\n",
        "\n",
        "The most common topology of a Keras model is a sequential stack of these types of layers, which maps one input to one output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMJPBfTjadlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of a simple network architecture, with a convolutional layer \n",
        "# and two fully connected layers\n",
        "\n",
        "# Creating the model\n",
        "model = models.Sequential()\n",
        "# Adding a convolutional layer and flattening its output\n",
        "model.add(Conv2D)\n",
        "model.add(layers.Flatten())\n",
        "# Adding a fully connected layer with output of 256 and ReLU activation function\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "# Adding a fully connected layer with output of 10 and sigmoid activation function\n",
        "model.add(layers.Dense(10, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG2GpWzZOD-J",
        "colab_type": "text"
      },
      "source": [
        "## 4. Compiling a model\n",
        "\n",
        "After defining the network architecture of your model, you must choose a loss function and optimizer for use during training.\n",
        "\n",
        "* The **loss function** is the function the model seeks to minimize during training, measuring the difference between values the model predicts and the expected target values for a given training dataset.\n",
        "* The **optimizer** handles the actual updating of the model in order to minimize the loss function. It implements a chosen variant of SGD.\n",
        "* We also choose what metrics (such as accuracy, loss) should be calculated so that we can monitor the performance of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_WC2-pN4AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    optimizer='rmsprop', \n",
        "    metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNcL4kdq8xL3",
        "colab_type": "text"
      },
      "source": [
        "Different loss functions are appropriate for different types of problems. The following table gives some appropriate loss functions for common ML problems.\n",
        "\n",
        "| Problem type              | Last layer activation  | Loss function              | \n",
        "|:-:                        |:-:                     |:-:                         |\n",
        "| Binary classification     | sigmoid                | binary_crossentropy        |\n",
        "| Multiclass, single-label  | softmax                | categorical_crossentropy   |\n",
        "| Mutlticlass, multi-label  | sigmoid                | binary_crossentropy        |\n",
        "| Regression to real values | none                   | mse                        |\n",
        "| Regression to \\[0,1\\]     | sigmoid                | mse or binary_crossentropy |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V--8CHwYOEUr",
        "colab_type": "text"
      },
      "source": [
        "## 5. Training a model\n",
        "\n",
        "After compiling a model, we use the `model.fit()` or `model.fit_generator()` functions, passing the training data, validation data, number of epochs, steps, and other parameters into the function. The output of these functions should be stored into a \"history\" variable, which will allow us to examine the loss/accuracy curves once training is complete.\n",
        "\n",
        "* **Overfitting:**\n",
        " * Overfitting is a common problem that occurs in training where a model is only able to make correct predictions on the training dataset, and is unable to generalize and make accurate predictions on data outside of that set.\n",
        " * Methods to mitigate overfitting:\n",
        "   * Early stopping - detect when overfitting is occuring (with a seperate validation dataset) and stop training before it can worsen\n",
        "   * Dropout - Randomly disable nodes within hidden layers\n",
        "   * Data augmentation - Generate more data (through slight modifications to the original training set) so that the model cannot overfit (which happens less easily with large, varied datasets)\n",
        "\n",
        "* **Underfitting**\n",
        " * Underfitting is essentially when a model is unable to train well enough to the input dataset. This occurs when a model is not complex enough for the dataset it is analyzing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QH3XC7O41bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Early stopping: \n",
        "# Instantiate an EarlyStopping callback.\n",
        "# Inputs: metric to monitor, what to search for (max, min, mse), verbosity,\n",
        "#         and patience (number of epochs where no improvement occurs before stopping)\n",
        "# Since this is a callback, it must be added to a list with any others, \n",
        "# then passed as an argument to model.fit()\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
        "cb_list = [es, ...]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LReP7Kop43Jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropout layer: drops a certain percentage of nodes, \n",
        "# chosen at random to prevent overfitting.\n",
        "model.add(layers.Dropout(0.3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U15Ewmnv44kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation as seen in HW 4:\n",
        "# each image is scaled/rotated by slight amounts to lessen chance of overfitting\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "# We can also do this for validation sets, but this is less necessary (since\n",
        "# they are not actually used for training)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Augmented data should then be added to a dataset which can be used by Keras.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX7lmBZezsaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model using datasets/callback list created in prior steps:\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50,\n",
        "    callbacks=cb_list\n",
        ")\n",
        "\n",
        "# If we do not do any data augmentation, we can use the simpler:\n",
        "history = model.fit(\n",
        "    training_data,\n",
        "    training_labels,\n",
        "    epochs=30,\n",
        "    batch_size = 128,\n",
        "    validation_data=(test_data, test_labels)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nBkwilmOElg",
        "colab_type": "text"
      },
      "source": [
        "## 6. Finetuning a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsZDLC_6Dq5M",
        "colab_type": "text"
      },
      "source": [
        "We can use pretrained models, especially convolutional models trained on specific datasets and tailored to different kinds of work, by adding them to our network as we would any layers. Prior to completing the training steps from the previous section, we should freeze the pretrained model and only train what we add onto it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFuFncoONuHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "# Example of a pretrained VGG16 CNN\n",
        "conv_base = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "# Set the pretrained model as untrainable\n",
        "conv_base.trainable = False\n",
        "\n",
        "# Create the model's architecture, inserting the pretrained model as well as any\n",
        "# other layers we choose to add.\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Continue to training (as seen in part 5)..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8x_ReCNTZ7Z",
        "colab_type": "text"
      },
      "source": [
        "After training this model, we should unfreeze some of the layers towards the end of the pre-trained model so that we can increase the accuracy of the model more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfN4za9_TxLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (This method of unfreezing works for VGG16, some pretrained CNNs have \n",
        "#  different labeling of layers and require adjustment)\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVUlMGx7UhX0",
        "colab_type": "text"
      },
      "source": [
        "We can then re-compile and re-train the model. We use a smaller learning rate this time, since we are just fine-tuning and do not need/want large changes to model accuracy anymore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eLcWaCLU6C2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    # Using a much smaller learning rate\n",
        "    optimizer=optimizers.RMSprop(lr=1e-5), \n",
        "    metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZgjvmGK9Uwm",
        "colab_type": "text"
      },
      "source": [
        "Sources:\n",
        "\n",
        "1. General concepts:\n",
        " * Professor's [Introductory slides](https://github.com/schneider128k/machine_learning_course/blob/master/slides/1_a_slides.pdf)\n",
        "\n",
        "2. Basic Concepts\n",
        " * [Wikipedia for Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)\n",
        " * [Wikipedia for Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
        " * [Wikipedia for Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
        " * Professor's [slides on Gradient Descent](https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_e_slides.pdf)\n",
        "\n",
        "3. Building a Model\n",
        " * Professor's [Keras Basics](https://github.com/schneider128k/machine_learning_course/blob/master/keras_basics.md)\n",
        " * [Explanation of Recurrent Networks](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
        " * [StackExchange answer with a very helpful animation of 2D convolution](https://stats.stackexchange.com/questions/154798/difference-between-kernel-and-filter-in-cnn/188216#188216)\n",
        "\n",
        "4. Compiling a Model\n",
        " * Professor's [Keras Basics](https://github.com/schneider128k/machine_learning_course/blob/master/keras_basics.md) (Table of loss functions is directly pulled from here)\n",
        "5. Training a Model\n",
        " * [Implementation of Early Stopping in Keras](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
        " * Professor's [Pretrained Convnet Finetuning Example](https://colab.research.google.com/drive/1F-RWvoxH8MmT7c1UmNy41iuOp-ejiLoF#scrollTo=Fh6gZSeAjF7c)\n",
        "6. Fine-tuning a Model\n",
        " * Professor's [Pretrained Convnet Finetuning Example](https://colab.research.google.com/drive/1F-RWvoxH8MmT7c1UmNy41iuOp-ejiLoF#scrollTo=Fh6gZSeAjF7c)"
      ]
    }
  ]
}